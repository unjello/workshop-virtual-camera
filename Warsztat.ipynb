{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5502690-a67c-40c1-8823-828bdbc6ec95",
   "metadata": {},
   "source": [
    "# Twoja własna wirtualna kamera - czyli sieci neuronowe dla każdego.\n",
    "Abstrakt: *Naucz się tworzyć swoje własne doświadczenia ze Sztuczną Inteligencją. Na warsztacie spojrzymy na obecny krajobraz SI z czysto pragmatycznego punktu widzenia, oraz pokażemy jak używać gotowych modeli w celu tworzenia nowych produktów, niezwłocznie przechodząc do budowy Waszego pierwszego narzędzia. Wirtualnej kamery nafaszerowanej sieciami neuronowymi.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c61020-d46d-4c49-8791-6bc4aa02a4e6",
   "metadata": {},
   "source": [
    "## Wykrywanie twarzy\n",
    "\n",
    "W pierwszej kolejności, spróbujmy wykryć twarz na różne sposoby."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6c1e91-dd6e-406c-99fb-cb7b825be24a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Instalacja zależności i importy\n",
    "`!` pozwala na uruchomienie linii komend i zainstalowanie zależności wprost z notatnika. Alternatywą byłoby dołączenie `requirements.txt`, ale zgodnie z zasadą *Reproducible Research*, starajmy się pisać notatniki tak, aby były w jak największym stopniu samowystarczalne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c33fa4c-e609-4a19-a3c8-72076645785e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install numpy opencv-python matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b42193f-b20f-4192-89bf-af5f638d60f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8e34ae-3a26-4f1e-bd3f-38fe07c3214d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Pobranie obrazu z kamerki\n",
    "Przy sporej większości przypadków związanych z *Computer Vision* używa się OpenCV. Nie inaczej postąpimy tutaj."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae0f218-545a-417f-9e1c-87d388c09c51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cap = cv.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    raise \"Nie można otworzyć kamerki\"\n",
    "ret, frame = cap.read()\n",
    "plt.imshow(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3ac551-ae89-407d-aaf6-02bb282f0a79",
   "metadata": {},
   "source": [
    "Obraz w OpenCV jest domyślnie trzymany jako BGR, czyli opisuje go kolejno Niebieski, Zielony i Czerwony, w przeciwieństwie do RGB powszechnego w grafice komputerowej. Powód, jak zazwyczaj, jest historyczny. Format BGR był popularny w swoim czasie wśród producentów kamer oraz oprogramowania do obróbki obrazu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85521a0-1fa5-4933-8f76-009d45526d41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.imshow(cv.cvtColor(frame, cv.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546424a3-467e-48b5-a85c-b196bd10a2a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Rozpoznawanie twarzy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c355bdb-ddd3-4728-97a5-724adb3244cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Kaskady Haar-a\n",
    "\n",
    "Nazwa wzięła się od tego że jądro splotu (*Convolution Kernel*) w algorytmie przypomina [falki Haar'a](https://en.wikipedia.org/wiki/Haar_wavelet) właśnie. Algorytm opisuje praca [*Rapid Object Detection using a Boosted Cascade of Simple Features*](https://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/viola-cvpr-01.pdf) z 2001. Sposób działania algorytmu możecie prześledzić na wideo poniżej:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00db959-3934-4884-aa2b-56b3ac46004a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<iframe src=\"https://player.vimeo.com/video/12774628?title=0&byline=0&portrait=0\" width=\"700\" height=\"394\" frameborder=\"0\" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0ad7c7-a777-4689-8838-1622298684fd",
   "metadata": {},
   "source": [
    "Kaskady Haar'a są wbudowane w OpenCV. To jest pierwszy, i w oczywiście najprostszy, sposób w jaki możemy używać *Machine Learning*. Sporo bibliotek ma popularne funkcjonalności już wbudowane. Najważniejsze w tym wypadku jest doczytać dokumentację, jakich danych model oczekuje na wejściu. W tym przypadku, musimy mieć obrazek w odcieniach szarości, o wyrównanym histogramie.\n",
    "\n",
    "Pliki modelu w tym wypadku są rozprowadzane z OpenCV, ale możemy je też pobrać bezpośrednio ze [strony](https://github.com/opencv/opencv/tree/master/data/haarcascades) i wersjonować lokalnie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b7d260-b3a8-49a4-840a-510652dd8c6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "haar_img = frame.copy()\n",
    "haar_img_gray = cv.cvtColor(haar_img, cv.COLOR_BGR2GRAY)\n",
    "haar_img_gray = cv.equalizeHist(haar_img_gray)\n",
    "face_cascade = cv.CascadeClassifier('haarcascade_frontalface_alt.xml')\n",
    "faces = face_cascade.detectMultiScale(haar_img_gray)\n",
    "for (x,y,w,h) in faces:\n",
    "        center = (x + w//2, y + h//2)\n",
    "        haar_img = cv.rectangle(haar_img, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "plt.imshow(cv.cvtColor(haar_img, cv.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e5f1eb-ac43-4f5a-a485-82851b32e06c",
   "metadata": {},
   "source": [
    "Kaskady Haar'a możemy wykorzystywać nie tylko do wykrywania twarzy. Są też modele do wykrywania oczu, części ciała, czy nawet rosyjskich tablic rejestracyjnych... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1034e584-c1d1-4954-ad4d-6440722c04c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eyes_cascade = cv.CascadeClassifier('haarcascade_eye_tree_eyeglasses.xml')\n",
    "for (x,y,w,h) in faces:\n",
    "    faceROI = haar_img_gray[y:y+h,x:x+w]\n",
    "    eyes = eyes_cascade.detectMultiScale(faceROI)\n",
    "    for (x2,y2,w2,h2) in eyes:\n",
    "        eye_center = (x + x2 + w2//2, y + y2 + h2//2)\n",
    "        radius = int(round((w2 + h2)*0.25))\n",
    "        haar_img = cv.circle(haar_img, eye_center, radius, (255, 0, 0 ), 4)\n",
    "plt.imshow(cv.cvtColor(haar_img, cv.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18edb4dc-6139-4da0-a6a0-f485e6bba0a0",
   "metadata": {},
   "source": [
    "### Dlib\n",
    "\n",
    "Dlib jest biblioteką algorytmów uczenia maszynowego, napisaną w C++. Posiada natomiast wiązania do pythona. Jest to bardzo popularne rozwiązanie: kod implementujący algorytmy, którego wydajność jest kluczowa, pisany jest w C/C++, CUDA czy częściowo w assemblerze, a API wystawiane jest do języka wyższego poziomu za pomocą tzw. *bindings*.\n",
    "\n",
    "Dlib posiada algorytm wykrywania twarzy oparty o konwolucyjne sieci neuronowe (*Convolutional Neural Networks*), ale nie nadaje się on do pracy w czasie rzeczywistym, dlatego przyjrzymy się innemu - wykrywaniu twarzy za pomocą Histogramu Gradientu Kierunkowego czyli *Histogram of Oriented Gradients*. Algorytm po raz pierwszy został opisany w [patencie](https://patents.google.com/patent/US4567610) z 1986, ale metoda nie zyskała popularności do czasu publikacji pracy [*Histograms of Oriented Gradients for Human Detection*](http://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf) w 2005."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b95345e-4525-479e-91ec-669aa6a16eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4c8d14-30aa-476b-84ec-52815e929112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f298bd6e-655a-457e-aeb8-434cd1826745",
   "metadata": {},
   "outputs": [],
   "source": [
    "dlib_img = frame.copy()\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "dlib_img_gray = cv.cvtColor(dlib_img, cv.COLOR_BGR2GRAY)\n",
    "faces = detector(dlib_img_gray, 1) \n",
    "for result in faces:\n",
    "    x = result.left()\n",
    "    y = result.top()\n",
    "    x1 = result.right()\n",
    "    y1 = result.bottom()\n",
    "    dlib_img = cv.rectangle(dlib_img, (x, y), (x1, y1), (0, 0, 255), 2)\n",
    "plt.imshow(cv.cvtColor(dlib_img, cv.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acab3937-0834-49ee-b296-ab377394aaf9",
   "metadata": {},
   "source": [
    "## Multi-task Cascaded Convolutional Networks\n",
    "\n",
    "MTCCN opublikował po raz pierwszy Kaipeng Zhang wraz z innymi, w artykule z 2016 pt. [“Joint Face Detection and Alignment Using Multi-task Cascaded Convolutional Networks”](https://arxiv.org/abs/1604.02878). Wykyrwa on nie tylko twarze, ale także 5 kluczowych punktów tzw. *facial landmarks*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2076604-a81f-46bf-a15e-8f3e205d0a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mtcnn.mtcnn import MTCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c88e985-4ca3-4682-824a-9dce09ba95aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcnn_img = frame.copy()\n",
    "detector = MTCNN()\n",
    "faces = detector.detect_faces(mtcnn_img)\n",
    "for result in faces:\n",
    "    x, y, w, h = result['box']\n",
    "    x1, y1 = x + w, y + h\n",
    "    mtcnn_img = cv.rectangle(mtcnn_img, (x, y), (x1, y1), (0, 0, 255), 2)\n",
    "plt.imshow(cv.cvtColor(mtcnn_img, cv.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da0aca4-2969-4b62-a6cd-6442060314da",
   "metadata": {},
   "source": [
    "### OpenCV DNN\n",
    "\n",
    "Stosunkowo niedawno OpenCV otrzymał swój własny moduł głębokiego uczenia. Dołączony model rozpoznawania twarzy, największą dokładność ma na obrazach w formacie BGR, pomniejszonych do rozmiaru 300x300 pikseli.\n",
    "\n",
    "Wszystkie modele dostępne w module DNN opisane są [tu](https://github.com/opencv/opencv/blob/master/samples/dnn/models.yml).\n",
    "    \n",
    "My będziemy używać 10 warstwowego modelu ResNET, z [opisem](https://github.com/opencv/opencv/blob/master/samples/dnn/face_detector/deploy.prototxt) i [wagami](https://github.com/opencv/opencv_3rdparty/raw/dnn_samples_face_detector_20170830/res10_300x300_ssd_iter_140000.caffemodel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a92a95-4a22-4164-a418-e89f3fed4eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_img = frame.copy()\n",
    "net = cv.dnn.readNetFromCaffe(\"deploy.prototxt\", \"res10_300x300_ssd_iter_140000.caffemodel\")\n",
    "h, w = dnn_img.shape[:2]\n",
    "blob = cv.dnn.blobFromImage(cv.resize(dnn_img, (300, 300)), 1.0, (300, 300), (104.0, 117.0, 123.0))\n",
    "net.setInput(blob)\n",
    "faces = net.forward()\n",
    "for i in range(faces.shape[2]):\n",
    "        confidence = faces[0, 0, i, 2]\n",
    "        if confidence > 0.5:\n",
    "            box = faces[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (x, y, x1, y1) = box.astype(\"int\")\n",
    "            dnn_img = cv.rectangle(dnn_img, (x, y), (x1, y1), (0, 0, 255), 2)\n",
    "plt.imshow(cv.cvtColor(dnn_img, cv.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea25609-ec78-4165-bdb0-fc8d94041c13",
   "metadata": {},
   "source": [
    "### BodyPix\n",
    "\n",
    "W 2019 roku, Google [wypuściło w świat](https://medium.com/tensorflow/introducing-bodypix-real-time-person-segmentation-in-the-browser-with-tensorflow-js-f1948126c2a0) swój model BodyPix. Nie dość, że potrafi on działać w czasie rzeczywistym, osiągając powyżej 20fps nawet na starszych sprzętach, to wykrywa nie tylko twarz, ale segmentuje całe ciało na aż 24 części!\n",
    "\n",
    "Używa albo MobileNetV1 albo ResNet50. Różnica między nimi jest głównie w rozmiarze, a co za tym idzie, wymaganiach sprzętowych i szybkości działania.\n",
    "\n",
    "\n",
    "| Architecture | quantBytes=4 | quantBytes=2 | quantBytes=1 |\n",
    "|--------------|:------------:|:------------:|:------------:|\n",
    "|ResNet50 | ~90MB | ~45MB | ~22MB|\n",
    "|MobileNetV1 (1.00) | ~13MB | ~6MB | ~3MB|\n",
    "|MobileNetV1 (0.75) | ~5MB | ~2MB | ~1MB|\n",
    "|MobileNetV1 (0.50) | ~2MB | ~1MB | ~0.6MB|\n",
    "\n",
    "Model do ściągnięcia [tu](https://storage.googleapis.com/tfjs-models/savedmodel/bodypix/mobilenet/float/050/model-stride16.json), a wagi [tu](https://storage.googleapis.com/tfjs-models/savedmodel/bodypix/mobilenet/float/050/group1-shard1of1.bin).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17a451b-3936-4475-8163-7a7b5dea4a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tf_bodypix[all]==0.3.5 tensorflow-gpu==2.4.1 tfjs_graph_converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d18410a-deba-407e-92b3-db960ac77d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tf_bodypix.api import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c415a3b0-14c6-4085-a586-11bb96e84116",
   "metadata": {},
   "outputs": [],
   "source": [
    "bodypix_model = load_model('mobilenet-float-50-model-stride16.json')\n",
    "bodypix_img = frame.copy()\n",
    "result = bodypix_model.predict_single(bodypix_img)\n",
    "mask = result.get_mask(threshold=0.5).numpy().astype(np.uint8)\n",
    "masked_image = result.get_colored_part_mask(mask, part_names=['left_face', 'right_face']).astype(np.uint8)\n",
    "final = cv.addWeighted(bodypix_img, 0.6, masked_image, 0.4, 2.0)\n",
    "plt.imshow(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139c4365-ae97-4225-9878-d3bbf10a46a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "contours, hierarchy = cv.findContours(cv.cvtColor(masked_image, cv.COLOR_RGB2GRAY), cv.RETR_EXTERNAL, cv.CHAIN_APPROX_NONE)\n",
    "for contour in contours:\n",
    "    box = cv.boundingRect(contour)\n",
    "    (x, y, w, h) = box\n",
    "    final = cv.rectangle(final, (x, y), (x+w, y+h), (0, 0, 255), 2)\n",
    "plt.imshow(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2ff533-271d-4735-8e9e-437450556004",
   "metadata": {},
   "source": [
    "#### Podsumowanie\n",
    "\n",
    "Kaskady Haara są dość przestarzałe i, gdyby zrobić takie porządne porównanie, z różnorodnym zbiorem testowym, to dawałyby najgorsze rezultaty. Dlib nie wykrywa twarzy na obrazkach mniejszych niż 80x80. Można je oczywiście powiększyć, ale to pogorszy jednocześnie wydajność, która nie jest mocną stroną Dliba i raczej nie nadaje się do pracy w czasie rzeczywistym poza komputerami stacjonarnymi. Moduł DNN z OpenCV i MTCNN radzą sobie podobnie, choć ten ostatni lepiej wypada gdy obrazy są duże.\n",
    "\n",
    "BodyPix wypada równie dobrze, a przy okazji oferuje dużo więcej informacji."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a491d2-48af-4523-b7c4-dbf020e3a942",
   "metadata": {},
   "source": [
    "## Wirtualne tło\n",
    "\n",
    "Jednym z przykładów, gdzie ta większa ilość informacji oferowana przez BodyPix może się przydać, to wirtualne tło, albo tzw. *virtual greenscreen*. Mając informację o korpusie postaci i twarzy, możemy jej użyć do usunięcia wszystkiego co znajduje się poza.\n",
    "\n",
    "W tym celu, załadujmy najpierw obraz, który będzie stanowił nasze tło."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc0801f-accd-498b-929c-4590c4aae9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "(fh, fw, fc) = frame.shape\n",
    "\n",
    "background = cv.imread('wawel-noca.jpg')\n",
    "(bh, bw, bc) = background.shape\n",
    "\n",
    "dif = bw if bh > bw else bh\n",
    "fdif = fw if fh > fw else fh\n",
    "\n",
    "x_pos = (bw - dif)//2\n",
    "y_pos = (bh - dif)//2\n",
    "\n",
    "background_mask = np.zeros((dif, dif, bc), dtype=background.dtype)\n",
    "background_mask[:dif, :dif, :] = background[y_pos:y_pos+dif, x_pos:x_pos+dif, :]\n",
    "\n",
    "background_squared = cv.resize(background_mask, (int(fdif*2), int(fdif*2)), cv.INTER_CUBIC)\n",
    "\n",
    "\n",
    "plt.imshow(cv.cvtColor(background_squared, cv.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4005496-dff8-4f91-8fe5-f2522aea3daf",
   "metadata": {},
   "source": [
    "Następnie pobierzemy maskę z BodyPix'a. Maska będzie miała `0` tam gdzie nie ma postaci, oraz `1` tam gdzie BodyPix postać wykrył. Możemy jej użyć do wycięcia postaci z naszej ramki z kamery. Do wymaskowania tła, będziemy musieli wykonać negację maski, a następnie dodamy tło i ramkę do siebie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25e0255-769f-4408-ac5f-5a147b7f6b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "(bh, bw, bc) = background_squared.shape\n",
    "\n",
    "x_pos = (bw - fw)//2\n",
    "y_pos = (bh - fh)//2\n",
    "\n",
    "background = background_squared[y_pos:y_pos+fh, x_pos:x_pos+fw,:]\n",
    "\n",
    "mask = result.get_mask(threshold=0.5).numpy().astype(np.uint8)\n",
    "masked_image = cv.bitwise_and(frame, frame, mask=mask)\n",
    "neg = np.add(mask, -1)\n",
    "inverse = np.where(neg == -1, 1, neg).astype(np.uint8)\n",
    "masked_background = cv.bitwise_and(background, background, mask=inverse)\n",
    "final = cv.add(masked_image, masked_background)\n",
    "plt.imshow(cv.cvtColor(final, cv.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f30900d-abbd-4daa-ad9f-418193558a4b",
   "metadata": {},
   "source": [
    "## Żywa kamera\n",
    "\n",
    "Spróbujmy teraz zrobić to samo, ale zamiast pracować na pojedynczym ujęciu z kamery, nałóżmy to na podgląd na żywo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afdc2fb-0fa3-4fe5-b41a-d9b55fd7cdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    result = bodypix_model.predict_single(frame)\n",
    "    (bh, bw, bc) = background_squared.shape\n",
    "\n",
    "    x_pos = (bw - fw)//2\n",
    "    y_pos = (bh - fh)//2\n",
    "\n",
    "    background = background_squared[y_pos:y_pos+fh, x_pos:x_pos+fw,:]\n",
    "\n",
    "    mask = result.get_mask(threshold=0.5).numpy().astype(np.uint8)\n",
    "    masked_image = cv.bitwise_and(frame, frame, mask=mask)\n",
    "    neg = np.add(mask, -1)\n",
    "    inverse = np.where(neg == -1, 1, neg).astype(np.uint8)\n",
    "    masked_background = cv.bitwise_and(background, background, mask=inverse)\n",
    "    final = cv.add(masked_image, masked_background)\n",
    "    \n",
    "    cv.imshow(\"Kamera\", final)\n",
    "    if cv.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be321e52-7411-4e86-9a22-8eac7f496325",
   "metadata": {},
   "source": [
    "## Paralaksa\n",
    "\n",
    "Kolejną rzeczą którą chciałbym zaproponować, jest efekt tzw. paralaksy. W skrócie polega on na ożywieniu tła za kamerą, tak, że wygląda ono jak trójwymiarowe. Przesuwając twarz, jakby oglądamy je z pod trochę innym kątem. \n",
    "\n",
    "W tym celu, będziemy śledzić twarz i jej pozycję. Do śledzenia, wybierzemy twarz o największej obwiedni. W rzeczywistości, algorym powinien być bardziej złożony. Przydatne byłoby choćby uchwycenie twarzy, aby w wypadku gdy aktualnie śledzona twarz oddali się i pojawi się kandydat o większej obwiedni, mimo wszystko śledzić dalej poprzednią, unikając widocznego przeskoku.\n",
    "\n",
    "Dla uchwyconej twarzy, obliczymy jej odchylenie od środka obrazu, żeby odpowiednio przesunąć tło.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34438cbe-ce84-48f9-a861-59ba51cf7fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "cap = cv.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    result = bodypix_model.predict_single(frame)\n",
    "    (bh, bw, bc) = background_squared.shape\n",
    "\n",
    "    mask = result.get_mask(threshold=0.5).numpy().astype(np.uint8)\n",
    "    face_masked_image = result.get_colored_part_mask(mask, part_names=['left_face', 'right_face']).astype(np.uint8)\n",
    "    contours, hierarchy = cv.findContours(cv.cvtColor(face_masked_image, cv.COLOR_RGB2GRAY), cv.RETR_EXTERNAL, cv.CHAIN_APPROX_NONE)\n",
    "    face = functools.reduce(lambda a,b: a if a[2]+a[3] > b[2]+b[3] else b, list(map(lambda a: cv.boundingRect(a), contours)))\n",
    "    \n",
    "    x_pos = (bw - fw)//2\n",
    "    y_pos = (bh - fh)//2\n",
    "    \n",
    "    if face:\n",
    "        (x, y, w, h) = face\n",
    "        face_x = x+w//2\n",
    "        face_y = y+h//2\n",
    "        x_pos -= int((fw-face_x)*0.25)\n",
    "        y_pos -= int((fh-face_y)*0.25)\n",
    "\n",
    "    background = background_squared[y_pos:y_pos+fh, x_pos:x_pos+fw,:]\n",
    "    \n",
    "    masked_image = cv.bitwise_and(frame, frame, mask=mask)\n",
    "        \n",
    "    neg = np.add(mask, -1)\n",
    "    inverse = np.where(neg == -1, 1, neg).astype(np.uint8)\n",
    "    masked_background = cv.bitwise_and(background, background, mask=inverse)\n",
    "    final = cv.add(masked_image, masked_background)\n",
    "    if face:\n",
    "        (x, y, w, h) = face\n",
    "        final = cv.rectangle(final, (x, y), (x+w, y+h), (0, 0, 255), 2)\n",
    "        final = cv.circle(final, (x+w//2, y+h//2), 5, (255, 0, 0 ), 4)\n",
    "            \n",
    "    cv.imshow(\"Kamera\", final)\n",
    "    if cv.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a87dce-9768-4a6e-b83f-a439ac622421",
   "metadata": {},
   "source": [
    "## Podsumowanie\n",
    "\n",
    "Pierwszą rzeczą którą zuważyliście zapewne uruchamiając przykład jest fakt, że obwiednia twarzy, a co za tym idzie wyliczony środek ciężkości, co ramkę zmienia delikatnie rozmiar. Wpływa to niestety na płynność przesuwania tła. Następnym krokiem, mogłoby być policzenie ważonej średniej ruchomej.\n",
    "\n",
    "Co jeszcze dalej? Mając powyższe modele, oraz moduł `pyvirtualcam`, możecie choćby spróbować sami zaimplementować swoją własną [Snap Kamerę](https://snapcamera.snapchat.com/).\n",
    "\n",
    "Owocnej zabawy!\n",
    "\n",
    "W razie pytań, nie obawiajcie się zagadnąć :) Znaleźć możecie mnie tu:\n",
    "\n",
    "- https://twitter.com/unjello\n",
    "- http://poly.work/unjello\n",
    "- https://www.linkedin.com/in/andrzejlichnerowicz/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}